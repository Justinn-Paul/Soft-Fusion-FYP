{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDFusion: Text-guided Generation (txt2shape)\n",
    "\n",
    "### TODO: add sample results or teaser images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first set up which gpu to use\n",
    "import os\n",
    "gpu_ids = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_ids}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sdfusion/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from IPython.display import Image as ipy_image\n",
    "from IPython.display import display\n",
    "from termcolor import colored, cprint\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from models.base_model import create_model\n",
    "from utils.util_3d import render_sdf, render_mesh, sdf_to_mesh, save_mesh_as_gif\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] SDFusionText2ShapeOption initialized.\n"
     ]
    }
   ],
   "source": [
    "# options for the model. please check `utils/demo_util.py` for more details\n",
    "from utils.demo_util import SDFusionText2ShapeOpt\n",
    "\n",
    "seed = 2023\n",
    "opt = SDFusionText2ShapeOpt(gpu_ids=gpu_ids, seed=seed)\n",
    "device = opt.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable proxy for huggingface\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 3, 16, 16, 16) = 12288 dimensions.\n",
      "\u001b[34m[*] VQVAE: weight successfully load from: /root/autodl-tmp/SDFusion/saved_ckpt/vqvae-snet-all.pth\u001b[0m\n",
      "\u001b[34m[*] weight successfully load from: /root/autodl-tmp/SDFusion/logs_home/continue-2024-04-05T17-15-19-sdfusion-txt2shape-text2shape-all-LR1e-5-clean-code/ckpt/df_steps-latest.pth\u001b[0m\n",
      "\u001b[34m[*] setting ddim_steps=100\u001b[0m\n",
      "\u001b[34m[*] Model has been created: SDFusion-Text2Shape-Model\u001b[0m\n",
      "\u001b[36m[*] \"SDFusion-Text2Shape-Model\" loaded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# initialize SDFusion model\n",
    "ckpt_path = '/root/autodl-tmp/SDFusion/logs_home/continue-2024-04-05T17-15-19-sdfusion-txt2shape-text2shape-all-LR1e-5-clean-code/ckpt/df_steps-latest.pth'\n",
    "vqvae_path = '/root/autodl-tmp/SDFusion/logs_home/continue-2024-03-27T11-26-32-vqvae-snet-all-res64-LR1e-4-T0.2-release/ckpt/vqvae_epoch-best.pth'\n",
    "vqvae_path = '/root/autodl-tmp/SDFusion/saved_ckpt/vqvae-snet-all.pth'\n",
    "opt.init_model_args(ckpt_path=ckpt_path, vq_ckpt_path=vqvae_path)\n",
    "\n",
    "SDFusion = create_model(opt)\n",
    "cprint(f'[*] \"{SDFusion.name()}\" loaded.', 'cyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch3d\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "\n",
    "def mesh_to_sdf(mesh, resolution=64, bounds=None, batch_size=1024):\n",
    "    device = mesh.device\n",
    "    # If bounds are not specified, compute them from the mesh\n",
    "    if bounds is None:\n",
    "        bounds = torch.stack([mesh.verts_packed().min(0)[0], mesh.verts_packed().max(0)[0]]).T\n",
    "        padding = (bounds[:, 1] - bounds[:, 0]) * 0.1\n",
    "        bounds[:, 0] -= padding\n",
    "        bounds[:, 1] += padding\n",
    "    \n",
    "    # Create a grid of points where the SDF will be evaluated\n",
    "    x = torch.linspace(bounds[0, 0], bounds[0, 1], resolution, device=device)\n",
    "    y = torch.linspace(bounds[1, 0], bounds[1, 1], resolution, device=device)\n",
    "    z = torch.linspace(bounds[2, 0], bounds[2, 1], resolution, device=device)\n",
    "    grid = torch.stack(torch.meshgrid(x, y, z), dim=-1).reshape(-1, 3)\n",
    "    \n",
    "    # Sample points from the mesh surface\n",
    "    num_samples = resolution ** 3  # Adjust the number of samples as needed\n",
    "    surface_points = sample_points_from_meshes(mesh, num_samples).squeeze(0)\n",
    "\n",
    "    # Initialize SDF tensor\n",
    "    sdf = torch.full((grid.size(0),), float('inf'), device=device)\n",
    "    \n",
    "    # Process each batch of grid points\n",
    "    for start in range(0, grid.size(0), batch_size):\n",
    "        end = min(start + batch_size, grid.size(0))\n",
    "        distances = torch.cdist(grid[start:end], surface_points)\n",
    "        min_distances, _ = torch.min(distances, dim=1)\n",
    "        sdf[start:end] = min_distances\n",
    "    \n",
    "    # Placeholder for inside/outside determination\n",
    "    sign = torch.ones_like(sdf)  # All outside for simplicity\n",
    "\n",
    "    # Convert distances to SDF values\n",
    "    sdf *= sign\n",
    "\n",
    "    # Reshape to the resolution grid\n",
    "    sdf = sdf.view(resolution, resolution, resolution)\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDFusion: text-guided generation (txt2shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([101, 1, 1, 64, 64, 64])\n",
      "sdfs loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:01<00:00, 70.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import pytorch3d\n",
    "from tqdm import tqdm\n",
    "\n",
    "# mesh_dir = '/root/autodl-tmp/SDFusion/test_results_nopre_small_dset/obj/'\n",
    "# mesh_dir = '/root/autodl-tmp/SDFusion/data/ShapeNet/ShapeNetCore.v1/01'\n",
    "# mesh_path = os.listdir(mesh_dir)\n",
    "# mesh_path = [os.path.join(mesh_dir, m_p) for m_p in mesh_path if m_p.endswith('obj')]\n",
    "# mesh_path = [os.path.join(mesh_dir, m_p, 'model.obj') for m_p in mesh_path if m_p.startswith('b')]\n",
    "# meshes = pytorch3d.io.load_objs_as_meshes(mesh_path, device=device)\n",
    "\n",
    "# print(\"meshes loaded\")\n",
    "\n",
    "# sdfs = []\n",
    "# for mesh in tqdm(meshes):\n",
    "#     sdf = mesh_to_sdf(mesh, resolution=64)\n",
    "#     sdfs.append(sdf)\n",
    "\n",
    "# sdfs = torch.stack(sdfs).unsqueeze(1).unsqueeze(1)\n",
    "# print(sdfs.shape)\n",
    "\n",
    "import h5py\n",
    "sdf_dir = \"/root/autodl-tmp/SDFusion/data/results_pretrained_small_dset/SDF_v1/resolution_64/01/\"\n",
    "sdf_paths = os.listdir(sdf_dir)\n",
    "\n",
    "sdfs = []\n",
    "for sub_dir in sdf_paths:\n",
    "    if not sub_dir.startswith('b'):\n",
    "        continue\n",
    "    sdf_path = os.path.join(sdf_dir, sub_dir, \"ori_sample_grid.h5\")\n",
    "    h5_f = h5py.File(sdf_path, 'r')\n",
    "    sdf = h5_f['pc_sdf_sample'][:].astype(np.float32)\n",
    "    sdf = torch.Tensor(sdf).view(1, 64, 64, 64).to(device)\n",
    "    sdf = sdf.unsqueeze(0).unsqueeze(0)\n",
    "    sdfs.append(sdf)\n",
    "\n",
    "sdfs = torch.cat(sdfs, dim=0)\n",
    "print(sdfs.shape)\n",
    "\n",
    "print(\"sdfs loaded\")\n",
    "\n",
    "codes = []\n",
    "SDFusion.eval()\n",
    "SDFusion.vqvae.eval()\n",
    "with torch.no_grad():\n",
    "    # SDFusion.vqvae.encode(sdf)\n",
    "    for sdf in tqdm(sdfs):\n",
    "        code = SDFusion.vqvae.encode_no_quant(sdf)\n",
    "        codes.append(code)\n",
    "\n",
    "# calculate mean and std for FID, but here we save first for later FID calculation\n",
    "codes = torch.cat(codes, dim=0)\n",
    "torch.save(codes, '/root/autodl-tmp/SDFusion/data/results_pretrained_small_dset/codes.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9bb85e944c303a90ba1b7f3901817f7bc3ecb5f736863b2299a6fa67a7b3c89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
